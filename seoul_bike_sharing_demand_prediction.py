# -*- coding: utf-8 -*-
"""seoul bike sharing demand prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OyP4tS6KPOH2WQr8noYtpYpXEOrcDIjq

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

"""# Loading Data"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("SeoulBikeData.csv", encoding="unicode_escape")
df.shape

df.head()

df.tail()

"""# Data Information"""

df.info()

df.describe(include="all").T

"""## Checking For Any Null Values"""

df.isnull().sum()

"""## Converting Date's data type
since the Date alone is not useful to us, as the machine cannot directly read it, we will break the Date down into day, month and year.
"""

df["Date"]=pd.to_datetime(df["Date"],dayfirst=True)
df["Weekday"]=df["Date"].dt.day_name()
df["Day"]=df["Date"].dt.day
df["Month"]=df["Date"].dt.month
df["Year"]=df["Date"].dt.year

df.drop("Date",axis=1,inplace=True)

df.info()

df.head()

"""# Exploratory Data Analysis (EDA)

- We shall generate a pair plot of the dataset.
- A pair plot is a grid of scatter plots where each numeric feature in the dataset is plotted against every other numeric feature.
- This helps visualize relationships and distributions within the data.
"""

sns.pairplot(df)

"""Now we shall generate some bar plots to visualize the total `Rented Bike Count` across different time periods



"""

plt.figure(figsize=(10,7))
Month=df.groupby("Month").sum().reset_index()
sns.barplot(x="Month",y="Rented Bike Count",data=Month)

plt.figure(figsize=(10,7))
Month=df.groupby("Day").sum().reset_index()
sns.barplot(x="Day",y="Rented Bike Count",data=Month)

plt.figure(figsize=(10,7))
Month=df.groupby("Hour").sum().reset_index()
sns.barplot(x="Hour",y="Rented Bike Count",data=Month)

"""Lets generate two bar plots to visualize the total `Rented Bike Count` in relation to `Holiday` and `Seasons`."""

plt.figure(figsize=(10,7))

sns.barplot(x="Holiday",y="Rented Bike Count",data=df)
sns.barplot(x="Seasons",y="Rented Bike Count",data=df)

"""`Rented Bike` Count v/s `Rainfall(mm)`"""

plt.figure(figsize=(40,7))
sns.barplot(x="Rainfall(mm)",y="Rented Bike Count",data=df)

"""This plot helps visualize the distribution of rented bike counts, showing how frequently different values or ranges of values appear in the data. It gives an idea of the central tendency, spread, and shape of the distribution."""

sns.displot(df["Rented Bike Count"])

"""- Taking square root to make the distribution of the data more symmetrical and closer to a normal distribution.
- This can be helpful for machine learning models that assume normally distributed data.
"""

sns.displot(np.sqrt(df["Rented Bike Count"]))

"""Lets calculate the skewness for all the numeric columns in the DataFrame `df`."""

df.skew(numeric_only=True)
#Positive is right skewed (tail on the right side) and negative is left skewed, near zero is bell shaped

"""Checking for Multicollinearity"""

plt.figure(figsize=(10,10))
#Select only numeric columns
df_numeric=df.select_dtypes(include=['number'])
#Create a heatmap
sns.heatmap(df_numeric.corr(),annot=True,cmap="coolwarm")

"""- After seeing the heatmap we realise that the temperature and dew temperature and correlated
- We can simply drop one of these, which has less correlation

We shall now calculate **VIF** (Variance Inflation Factor)

---


Why Calculate **VIF**?

- VIF helps detect multicollinearity among features.
- A high VIF (typically above 5 or 10) suggests that a feature is highly
correlated with other features,which can inflate standard errors and lead to less reliable statistical estimates in a model.
"""

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

"""variance_inflation_model is a function from statsmodels, which calculates the Variance Inflation Factor (VIF) for each feature in a dataset.
VIF helps detect multicollinearity (i.e., when features are highly correlated) in regression analysis,
which can lead to unstable and misleading regression coefficients."""

# Function to calculate VIF
def get_vif(df):
    vif = pd.DataFrame() #Creates a new empty data frame named vif, which will store the VIF values.
    vif["variables"] = df.columns #Sets the "variables" column of vif to list the names of columns in df, which are the features for which we’ll calculate VIF.
    vif["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])] #This line calculates VIF for each column in df
    return vif

# Exclude columns that shouldn't be included in VIF calculation
not_for_vif = ["Day", "Month", "Year", "Rented Bike Count"]

# Ensure only numeric columns are selected after excluding specific columns
df_vif = df[[i for i in df.describe().columns if i not in not_for_vif]].select_dtypes(include=['number'])

# Calculate VIF
vif_result = get_vif(df_vif)
print(vif_result)

df.drop(["Dew point temperature(°C)"],axis=1,inplace=True)

"""### Encoding"""

df.info()

cat_features=["seasons","Holiday","Weekday","Functioning Day"]
df["Holiday"].value_counts()

df["Functioning Day"].value_counts()

"""Next we'll do some transformations


---


These transformations prepare the categorical data in Holiday,
Functioning Day, Seasons, and Weekday for use in machine learning models
by converting text categories into numerical representations, which most models require.

- **One-Hot Encoding**: This technique converts categorical variables with multiple categories into multiple binary columns, where each column represents a category.
Each row will have a 1 in the column for the category it belongs to and 0 in the others.


---


`drop_first=True`: This option drops the first category in each one-hot encoding transformation to avoid multicollinearity.
For example, if Seasons had categories `["Winter", "Spring", "Summer", "Fall"]`,
using `drop_first=True` would create three new columns instead of four,
encoding only ***Spring, Summer, and Fall***. If a row has `0`s in all these columns,
it indicates ***Winter*** (the dropped category).
"""

df["Holiday"]=df["Holiday"].map({"No Holiday":0,"Holiday":1})
#These lines convert the categorical values in the Holiday and Functioning Day columns into binary numerical values.
df["Functioning Day"]=df["Functioning Day"].map({"No":0,"Yes":1})


df_seasons=pd.get_dummies(df["Seasons"],drop_first=True)
#Here, one-hot encoding is applied to the Seasons and Weekday columns to create new binary columns for each unique category within these columns.
df_weekday=pd.get_dummies(df["Weekday"],drop_first=True)

df.info()

df=pd.concat([df,df_seasons,df_weekday],axis=1)

df.info()

df.drop(["Seasons","Weekday"],axis=1,inplace=True)

df.info()

"""Now our data is ready, we shall now split our data into two segments- `train` and `test`

# Splitting Data for training and testing
"""

""" X contains all the columns in df except for "Rented Bike Count",
which is dropped because it is the target variable we want to predict.
y contains only the "Rented Bike Count" column,
which represents the values we’re trying to predict."""


X=df.drop("Rented Bike Count",axis=1)
y=df["Rented Bike Count"]


"""  The train_test_split function from sklearn.model_selection splits
the data into training and testing sets.
test_size=0.2 specifies that 20% of the data should be set aside for testing,
while 80% will be used for training.
random_state=7 ensures reproducibility by setting a fixed seed for the random split.
Using the same random state will produce the same split every time the code is run. """


X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=7)
print("Shape of X_train:",X_train.shape)
print("Shape of X_test:",X_test.shape)
print("Shape of y_train:",y_train.shape)
print("Shape of y_test:",y_test.shape)

"""### Scaling
Standardizing the data often helps improve the performance and stability of machine learning models, especially those that are sensitive to the scale of the input data, such as regression algorithms, neural networks, and distance-based models (like KNN).
"""

sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

sc.mean_

sc.scale_

"""# Training ML Models

## Linear Regression Model
"""

from sklearn.linear_model import LinearRegression
"""  This line creates an instance of the LinearRegression class from sklearn.linear_model.

LinearRegression is a simple machine learning model that finds the best-fitting line through
the data by minimizing the sum of squared differences between the predicted and actual values.
This best-fit line is defined by the model’s weights (slopes) and intercepts,
which represent the relationship between the input features and the target variable. """

lr= LinearRegression()

"""  .fit(X_train, y_train) trains the linear regression model using the training data.
The method calculates the optimal weights (or coefficients) and intercept for the linear
relationship between X_train (the features) and y_train (the target variable).
During this step, the model learns how each feature in X_train affects the target variable
y_train. Once trained, this linear relationship can be used to predict values for new,
unseen data. """

lr.fit(X_train_scaled,y_train)

""" The predictions in y_pred are crucial for assessing the model’s performance.
By comparing y_pred with y_test (actual values),
you can evaluate how accurately the linear regression model is predicting the target variable,
often using metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared (R²). """

y_pred = lr.predict(X_test_scaled)

y_pred

"""### Model Evaluation"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

MSE= mean_squared_error(y_test, y_pred)
RMSE= np.sqrt(MSE)
MAE= mean_absolute_error(y_test, y_pred)
R2= r2_score(y_test, y_pred)

print(f"MSE: {MSE}")
print(f"RMSE: {RMSE}")
print(f"MAE: {MAE}")
print(f"R2: {R2}")

""" The R² score of ~0.561 suggests the model explains about 56% of the variance
in the target variable, indicating room for improvement in predictive performance. """

#Creating a function, so that we can run it easily.
def get_metrics(y_true, y_pred, model_name):
    MSE= mean_squared_error(y_test, y_pred)
    RMSE= np.sqrt(MSE)
    MAE= mean_absolute_error(y_test, y_pred)
    R2= r2_score(y_test, y_pred)

    print(f"{model_name} : ['MSE':{round(MSE,3)}, 'RMSE':{round(RMSE,3)}, 'MAE':{round(MAE,3)}, 'R2':{round(R2,3)}]")

get_metrics(y_test, y_pred, "LinearRegression")

"""## Train Multiple Models

"""

# Models that NEED scaled data
rir = Ridge().fit(X_train_scaled, y_train)
y_pred_rir = rir.predict(X_test_scaled)

lar = Lasso().fit(X_train_scaled, y_train)
y_pred_lar = lar.predict(X_test_scaled)

# Polynomial Regression (also scale-sensitive)
poly = PolynomialFeatures(2)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

poly_r = LinearRegression().fit(X_train_poly, y_train)
y_pred_poly = poly_r.predict(X_test_poly)

svr = SVR().fit(X_train_scaled, y_train)
y_pred_svr = svr.predict(X_test_scaled)

knnr = KNeighborsRegressor().fit(X_train_scaled, y_train)
y_pred_knnr = knnr.predict(X_test_scaled)

# Decision Tree — scaling not needed
dtr = DecisionTreeRegressor().fit(X_train, y_train)
y_pred_dtr = dtr.predict(X_test)

# Random Forest — not scale-sensitive
rfr = RandomForestRegressor().fit(X_train, y_train)
y_pred_rfr = rfr.predict(X_test)

# XGBoost — not scale-sensitive
xgbr = XGBRegressor().fit(X_train, y_train)
y_pred_xgbr = xgbr.predict(X_test)

get_metrics(y_test, y_pred_rir, "Ridge")
get_metrics(y_test, y_pred_lar, "Lasso")
get_metrics(y_test, y_pred_poly, "PolynomialFeatures")
get_metrics(y_test, y_pred_svr, "SVR")
get_metrics(y_test, y_pred_knnr, "KNN")
get_metrics(y_test, y_pred_dtr, "DecisionTreeRegressor")
get_metrics(y_test, y_pred_rfr, "RandomForestRegressor")
get_metrics(y_test, y_pred_xgbr, "XGBRegressor")

"""# Visualising Model Predictions"""

plt.scatter(y_test, y_pred)
plt.xlabel("Ground Truth")
plt.ylabel("Prediction")
plt.show()

plt.scatter(y_test, y_pred_rir)
plt.xlabel("Ground Truth")
plt.ylabel("Prediction")
plt.show()

plt.scatter(y_test, y_pred_xgbr)
plt.xlabel("Ground Truth")
plt.ylabel("Prediction")
plt.show()

plt.scatter(y_test, y_pred_dtr)
plt.xlabel("Ground Truth")
plt.ylabel("Prediction")
plt.show()

"""# Hyperparamter Tuning

- Hyperparameter tuning is the process of finding the best set of hyperparameters for a machine learning model to optimize its performance.

- Think of hyperparameters as the "settings" of a machine learning algorithm that are not learned from the data itself, but are set before the training process begins. Examples include the number of trees in a Random Forest, the learning rate in a gradient boosting model, or the penalty parameter in a regression model.

- The goal of hyperparameter tuning is to find the combination of these settings that results in the best performance of your model on unseen data, often measured using metrics like accuracy, precision, recall, or R-squared.

### for Random Forest Regressor
"""

from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]

# Number of features to consider at every split
max_features = [None, 'sqrt']  # Use None instead of 'auto' for compatibility with newer versions

# Maximum number of levels allowed in each decision tree
max_depth = [int(x) for x in np.linspace(10, 120, num=12)]

# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]

# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]

# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'bootstrap': bootstrap
}

import time
start_time =time.time()

rf=RandomForestRegressor()
#Random search of parameters, using 3 fold cross validation,
#search across 100 different combinations and use all available cores
rf_random=RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, cv=3, verbose=2, random_state=42, n_jobs=-1)
#Fit the random search model
rf_random.fit(X_train, y_train)
y_pred_rf_random = rf_random.predict(X_test)

print("Time taken to train using randomized search :",time.time()-start_time)

get_metrics(y_test, y_pred_rf_random, "RandomForestRegressor Fine Tuning")

rf_random.best_params_

rf_tuned= RandomForestRegressor (n_estimators= 800,
 min_samples_split= 2,
 min_samples_leaf=1,
 max_features= None,
 max_depth= 100,
 bootstrap= True)


rf_tuned.fit(X_train, y_train)
y_pred_rf_tuned=rf_tuned.predict(X_test)

get_metrics(y_test, y_pred_rf_tuned, "RandomForestRegressor with Best Parameters")

"""### for XGBOOST Regressor"""

from sklearn.model_selection import RandomizedSearchCV

import time
start_time = time.time()

params = { 'max_depth': [3, 5, 6, 10, 15, 20],
           'learning_rate': [0.01, 0.1, 0.2, 0.3],
           'subsample': np.arange(0.5, 1.0, 0.1),
           'colsample_bytree': np.arange(0.4, 1.0, 0.1),
           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),
           'n_estimators': [100, 500, 1000]}


xgbr = XGBRegressor(seed = 20)
rscv = RandomizedSearchCV(estimator=xgbr,
                         param_distributions=params,
                         scoring='neg_mean_squared_error',
                         n_iter=25,
                          cv=5,
                         verbose=1)

rscv.fit(X_train, y_train)

y_pred_xgb_random = rscv.predict(X_test)

get_metrics(y_test, y_pred_xgb_random, "XGBRegressor With Best Parameters")

print("Time taken to training using randomize search : ", time.time()-start_time)

print("Best parameters:", rscv.best_params_)

xgbr = XGBRegressor(subsample= 0.6,
n_estimators= 500,
max_depth= 5,
learning_rate= 0.1,
colsample_bytree= 0.7999999999999999,
colsample_bylevel= 0.8999999999999999,
                   seed=20)

xgbr.fit(X_train, y_train)

y_pred_tuned = xgbr.predict(X_test)

get_metrics(y_test, y_pred_tuned, "XGBRegressor With Best Parameters")

"""# Saving the best ML Model"""

import pickle
import os
import joblib
# Create folder if not present
os.makedirs("models", exist_ok=True)

# Save best estimator from RandomizedSearchCV
model_path = os.path.join("models", "xgboost_best_model_r2_0_950.pkl")
pickle.dump(rscv.best_estimator_, open(model_path, "wb"))

# Save feature list
feature_list_path = os.path.join("models", "feature_order.pkl")
joblib.dump(X_train.columns.tolist(), feature_list_path)

print(f"Model saved to {model_path}")
print(f"Feature list saved to {feature_list_path}")

"""# Visualising Results"""

def plot_model_results(y_true, y_pred, save=False, save_dir="plots", model_name="xgboost"):
    """
    Generates 3 diagnostic plots:
    1. Predicted vs Actual scatter
    2. Residual plot
    3. Line plot of predictions vs actuals (sample)

    Params:
        y_true (array-like): Ground truth values
        y_pred (array-like): Predicted values
        save (bool): Whether to save plots as images
        save_dir (str): Folder to save plots
        model_name (str): Used in filenames if saving
    """
    os.makedirs(save_dir, exist_ok=True)

    # 1. Scatter Plot
    plt.figure(figsize=(8, 6))
    plt.scatter(y_true, y_pred, alpha=0.5, color='royalblue')
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    plt.xlabel("Actual Rented Bike Count")
    plt.ylabel("Predicted Rented Bike Count")
    plt.title(f"{model_name} Predictions vs Actuals")
    plt.grid(True)
    if save:
        plt.savefig(f"{save_dir}/{model_name}_pred_vs_actual.png")
    plt.show()

    # 2. Residual Plot
    residuals = y_true - y_pred
    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals, alpha=0.5, color='orange')
    plt.axhline(0, linestyle='--', color='red')
    plt.xlabel("Predicted Rented Bike Count")
    plt.ylabel("Residuals")
    plt.title(f"{model_name} Residuals")
    plt.grid(True)
    if save:
        plt.savefig(f"{save_dir}/{model_name}_residuals.png")
    plt.show()

    # 3. Actual vs Predicted (first 100 points)
    plt.figure(figsize=(12, 6))
    plt.plot(np.arange(100), y_true[:100].values, label='Actual', marker='o')
    plt.plot(np.arange(100), y_pred[:100], label='Predicted', marker='x')
    plt.title(f"{model_name} Predictions vs Actuals (First 100 Points)")
    plt.xlabel("Sample Index")
    plt.ylabel("Rented Bike Count")
    plt.legend()
    plt.grid(True)
    if save:
        plt.savefig(f"{save_dir}/{model_name}_lineplot.png")
    plt.show()

plot_model_results(y_test, y_pred_tuned, save=True, model_name="xgboost_tuned")

"""# Lets make an interface for us to enter data"""

!pip install gradio --quiet

"""*Code for a Simple Gradio Interface*


---


Remove the comments when you run this-

import gradio as gr
import pandas as pd
import pickle
import joblib
import os

# Load trained model and feature list
model = pickle.load(open("models/xgboost_best_model_r2_0_950.pkl", "rb"))
feature_order = joblib.load(open("models/feature_order.pkl", "rb"))

# Define user-input features (must match what user sees in UI)
input_features = [
    "Hour", "Temperature(°C)", "Humidity(%)", "Wind speed (m/s)", "Visibility (10m)",
    "Solar Radiation (MJ/m2)", "Rainfall(mm)", "Snowfall (cm)",
    "Holiday", "Functioning Day",
    "Spring", "Summer", "Winter",
    "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday",
    "Day", "Month", "Year"
]

# Predict function
def predict_bike_demand(*inputs):
    user_input = dict(zip(input_features, inputs))
    df = pd.DataFrame([user_input])
    df = df.reindex(columns=feature_order, fill_value=0)  # Align with model

    prediction = model.predict(df)[0]
    return int(prediction)

# Gradio input controls
inputs = [
    gr.Slider(0, 23, value=9, label="Hour"),
    gr.Slider(-10, 40, value=15, label="Temperature (°C)"),
    gr.Slider(0, 100, value=60, label="Humidity (%)"),
    gr.Slider(0, 10, value=1.5, label="Wind Speed (m/s)"),
    gr.Slider(0, 2000, value=1000, label="Visibility (10m)"),
    gr.Slider(0, 3, value=0.2, label="Solar Radiation (MJ/m2)"),
    gr.Slider(0, 50, value=0.0, label="Rainfall (mm)"),
    gr.Slider(0, 10, value=0.0, label="Snowfall (cm)"),
    gr.Radio([0, 1], label="Is Holiday? (1=Yes, 0=No)"),
    gr.Radio([0, 1], label="Functioning Day? (1=Yes, 0=No)"),
    gr.Radio([0, 1], label="Is Spring?"),
    gr.Radio([0, 1], label="Is Summer?"),
    gr.Radio([0, 1], label="Is Winter?"),
    gr.Radio([0, 1], label="Is Monday?"),
    gr.Radio([0, 1], label="Is Tuesday?"),
    gr.Radio([0, 1], label="Is Wednesday?"),
    gr.Radio([0, 1], label="Is Thursday?"),
    gr.Radio([0, 1], label="Is Friday?"),
    gr.Radio([0, 1], label="Is Saturday?"),
    gr.Radio([0, 1], label="Is Sunday?"),
    gr.Slider(1, 31, value=15, label="Day"),
    gr.Slider(1, 12, value=7, label="Month"),
    gr.Slider(2017, 2018, value=2018, label="Year")
]

# Launch the Gradio app
gr.Interface(
    fn=predict_bike_demand,
    inputs=inputs,
    outputs=gr.Number(label="Predicted Bike Count"),
    title="🚲 Bike Demand Predictor",
    description="Enter weather, time, and calendar info to estimate hourly bike rental demand in Seoul."
).launch(debug=True)

import matplotlib.pyplot as plt

# Define a sample input dictionary
sample_input = {
    "Hour": 10,
    "Temperature(°C)": 20.0,
    "Humidity(%)": 55.0,
    "Wind speed (m/s)": 1.5,
    "Visibility (10m)": 1800,
    "Solar Radiation (MJ/m2)": 0.5,
    "Rainfall(mm)": 0.0,
    "Snowfall (cm)": 0.0,
    "Holiday": 0,
    "Functioning Day": 1,
    "Spring": 0,
    "Summer": 1,
    "Winter": 0,
    "Monday": 0,
    "Tuesday": 0,
    "Wednesday": 0,
    "Thursday": 1,
    "Friday": 0,
    "Saturday": 0,
    "Sunday": 0,
    "Day": 10,
    "Month": 7,
    "Year": 2018
}

# Convert to DataFrame and reorder columns
input_df = pd.DataFrame([sample_input])
input_df = input_df.reindex(columns=feature_order, fill_value=0)

# Predict
prediction = model.predict(input_df)[0]

# Create simple visual
plt.figure(figsize=(6, 4))
plt.text(0.5, 0.6, f"Predicted Bike Count:\n{int(prediction)}", fontsize=20, ha='center')
plt.axis('off')
plt.title("XGBoost Prediction Example")
plt.savefig("gradio_sample_output.png", dpi=229)
plt.show()
"""

